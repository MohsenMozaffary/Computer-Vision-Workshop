{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from Loader import TrainLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from Network import Net\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import torchvision.utils as utils\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset at https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection\n",
    "main_dir = \"D:\\\\Workshops\\\\CNN_workshop\\\\workshop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34acc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(main_dir, train_ratio, val_ratio, test_ration):\n",
    "    yes_dir = os.path.join(main_dir, \"archive\", \"yes\")\n",
    "    no_dir = os.path.join(main_dir, \"archive\", \"no\")\n",
    "    yes_names = os.listdir(yes_dir)\n",
    "    no_names = os.listdir(no_dir)\n",
    "    len_yes = len(yes_names)\n",
    "    len_no = len(no_names)\n",
    "    \n",
    "    yes_train_names = yes_names[:int(train_ratio*len_yes)]\n",
    "    no_train_names = no_names[:int(train_ratio*len_no)]\n",
    "    \n",
    "    yes_val_names = yes_names[int(train_ratio*len_yes):int((train_ratio+val_ratio)*len_yes)]\n",
    "    no_val_names = no_names[int(train_ratio*len_no):int((train_ratio+val_ratio)*len_no)]\n",
    "    \n",
    "    yes_test_names = yes_names[int((train_ratio+val_ratio)*len_yes):]\n",
    "    no_test_names = no_names[int((train_ratio+val_ratio)*len_no):]\n",
    "    \n",
    "    # creating data folder\n",
    "    data_dir = os.path.join(main_dir, \"data\")\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "        os.mkdir(train_dir)\n",
    "        os.mkdir(val_dir)\n",
    "        os.mkdir(test_dir)\n",
    "        \n",
    "    # copying train data\n",
    "    for yes_train_sample in yes_train_names:\n",
    "        source = os.path.join(yes_dir, yes_train_sample)\n",
    "        destination = os.path.join(train_dir, yes_train_sample)\n",
    "        shutil.copy(source, destination)\n",
    "    for no_train_sample in no_train_names:\n",
    "        source = os.path.join(no_dir, no_train_sample)\n",
    "        destination = os.path.join(train_dir, no_train_sample)\n",
    "        shutil.copy(source, destination)  \n",
    "        \n",
    "    for yes_val_sample in yes_val_names:\n",
    "        source = os.path.join(yes_dir, yes_val_sample)\n",
    "        destination = os.path.join(val_dir, yes_val_sample)\n",
    "        shutil.copy(source, destination)\n",
    "    for no_val_sample in no_val_names:\n",
    "        source = os.path.join(no_dir, no_val_sample)\n",
    "        destination = os.path.join(val_dir, no_val_sample)\n",
    "        shutil.copy(source, destination) \n",
    "        \n",
    "    for yes_test_sample in yes_test_names:\n",
    "        source = os.path.join(yes_dir, yes_test_sample)\n",
    "        destination = os.path.join(test_dir, yes_test_sample)\n",
    "        shutil.copy(source, destination)\n",
    "    for no_test_sample in no_test_names:\n",
    "        source = os.path.join(no_dir, no_test_sample)\n",
    "        destination = os.path.join(test_dir, no_test_sample)\n",
    "        shutil.copy(source, destination)  \n",
    "    \n",
    "    \n",
    "    return yes_names, no_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, n = data_split(main_dir, 0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6125fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a89c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:\\\\Workshops\\\\CNN_workshop\\\\workshop\\\\data\\\\train\"\n",
    "val_path = \"D:\\\\Workshops\\\\CNN_workshop\\\\workshop\\\\data\\\\val\"\n",
    "test_path = \"D:\\\\Workshops\\\\CNN_workshop\\\\workshop\\\\data\\\\test\"\n",
    "train_loader_obj = TrainLoader(train_path, y, n)\n",
    "train_loader = DataLoader(dataset=train_loader_obj, batch_size = batch_size, shuffle=True)\n",
    "val_loader_obj = TrainLoader(val_path, y, n)\n",
    "val_loader = DataLoader(val_loader_obj, batch_size = 32, shuffle = True)\n",
    "test_loader_obj = TrainLoader(test_path, y, n)\n",
    "test_loader = DataLoader(test_loader_obj, batch_size = 32, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b841453",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(val_loader)\n",
    "images, labels = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40416c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64af079",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc730d",
   "metadata": {},
   "source": [
    "# Creating Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4199bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "device = \"cuda\"\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the neural network\n",
    "for epoch in range(500):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "        labels = labels.to(device)\n",
    "        inputs = inputs.to(device).float()\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d347f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_name = \"Conv_weights.pt\"\n",
    "#torch.save(net.state_dict(), weight_name)\n",
    "net.load_state_dict(torch.load(\"Conv_weights.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "iterator = iter(test_loader)\n",
    "images, labels = next(iterator)\n",
    "\n",
    "inputs = images.to(device).float()\n",
    "inputs = inputs.permute(0, 3, 1, 2)\n",
    "with torch.no_grad():\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "print(labels)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = predicted.to(device) - labels.to(device)\n",
    "acc = 1- sum(abs(diff))/len(predicted)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = cv2.imread(\"D:\\\\Workshops\\\\CNN_workshop\\\\workshop\\\\data\\\\test\\\\Y82.jpg\")\n",
    "plt.imshow(input_image)\n",
    "input_image = input_image/255.0\n",
    "input_image = (input_image - np.mean(input_image))/np.std(input_image)\n",
    "input_image = torch.from_numpy(input_image)\n",
    "input_image = input_image.unsqueeze(0).float().to(device)\n",
    "input_image = input_image.permute(0, 3, 1, 2)\n",
    "conv3_output = net.conv3(net.pool(nn.functional.relu(net.conv2(net.pool(nn.functional.relu(net.conv1(input_image))))))).detach()\n",
    "conv3_output = conv3_output.squeeze()\n",
    "conv3_output = conv3_output.detach().to('cpu')\n",
    "conv3_output = np.array(conv3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_output = net.conv1(input_image).detach()\n",
    "conv1_output = conv1_output.squeeze()\n",
    "conv1_output = conv1_output.detach().to('cpu')\n",
    "conv1_output = np.array(conv1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c72e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(conv1[12], vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea30e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv1[5], vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c644f52",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209bcc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReNet_model import ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a02c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet()\n",
    "device = \"cuda\"\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a9ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the neural network\n",
    "epoch_acc = 0\n",
    "iterator = iter(val_loader)\n",
    "images_val, labels_val = next(iterator)\n",
    "for epoch in range(500):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "        labels = labels.to(device)\n",
    "        inputs = inputs.to(device).float()\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    net.eval()\n",
    "\n",
    "    inputs = images_val.to(device).float()\n",
    "    inputs = inputs.permute(0, 3, 1, 2)\n",
    "    with torch.no_grad():\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "    \n",
    "    diff = predicted.to(device) - labels_val.to(device)\n",
    "    acc = 1- sum(abs(diff))/len(predicted)\n",
    "    print(\"Accuracy of epoch {} is {}\".format(epoch+1, acc*100))\n",
    "    \n",
    "    if acc > epoch_acc:\n",
    "        weight_name = \"best_model.pt\"\n",
    "        torch.save(net.state_dict(), weight_name)\n",
    "        epoch_acc = acc\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"best_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b401cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "iterator = iter(test_loader)\n",
    "images, labels = next(iterator)\n",
    "\n",
    "inputs = images.to(device).float()\n",
    "inputs = inputs.permute(0, 3, 1, 2)\n",
    "with torch.no_grad():\n",
    "    outputs = net(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "print(labels)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a307730",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = predicted.to(device) - labels.to(device)\n",
    "acc = 1- sum(abs(diff))/len(predicted)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = cv2.imread(\"D:\\\\Workshops\\\\CNN_workshop\\\\workshop\\\\data\\\\test\\\\Y82.jpg\")\n",
    "input_image = input_image/255.0\n",
    "input_image = (input_image - np.mean(input_image))/np.std(input_image)\n",
    "input_image = torch.from_numpy(input_image)\n",
    "input_image = input_image.unsqueeze(0).float().to(device)\n",
    "input_image = input_image.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c80ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.resnet[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3cd01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_output = net.resnet[:4][0](input_image).detach()\n",
    "conv1_output = conv1_output.squeeze()\n",
    "conv1_output = conv1_output.detach().to('cpu')\n",
    "conv1_output = np.array(conv1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5197bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv1_output[1], vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27eb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv1_output[19], vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv1_output[24], vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc710a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(conv1_output[28], vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca39466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
